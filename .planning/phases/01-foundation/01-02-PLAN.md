---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - lobster/core/vector/embeddings/sapbert.py
  - lobster/core/vector/backends/chromadb_backend.py
autonomous: true
requirements:
  - EMBED-02
  - EMBED-05
  - INFRA-04
  - INFRA-08

must_haves:
  truths:
    - "SapBERTEmbedder loads model lazily on first embed_text() call, not at import or __init__ time"
    - "SapBERTEmbedder uses CLS-token pooling (not mean pooling) for SapBERT model"
    - "ChromaDBBackend creates ~/.lobster/vector_store/ directory with parents=True on initialization"
    - "ChromaDBBackend uses cosine HNSW space for all collections"
    - "Importing sapbert.py without sentence-transformers raises ImportError with 'pip install lobster-ai[vector-search]' message"
    - "Importing chromadb_backend.py without chromadb raises ImportError with 'pip install lobster-ai[vector-search]' message"
  artifacts:
    - path: "lobster/core/vector/embeddings/sapbert.py"
      provides: "SapBERTEmbedder with lazy loading, CLS pooling, 768d embeddings"
      contains: "class SapBERTEmbedder"
    - path: "lobster/core/vector/backends/chromadb_backend.py"
      provides: "ChromaDBBackend with PersistentClient, cosine HNSW, versioned collections"
      contains: "class ChromaDBBackend"
  key_links:
    - from: "lobster/core/vector/embeddings/sapbert.py"
      to: "lobster/core/vector/embeddings/base.py"
      via: "extends BaseEmbedder ABC"
      pattern: "class SapBERTEmbedder\\(BaseEmbedder\\)"
    - from: "lobster/core/vector/backends/chromadb_backend.py"
      to: "lobster/core/vector/backends/base.py"
      via: "extends BaseVectorBackend ABC"
      pattern: "class ChromaDBBackend\\(BaseVectorBackend\\)"
---

<objective>
Implement the two concrete adapters: SapBERTEmbedder (SapBERT model with lazy loading and CLS pooling) and ChromaDBBackend (PersistentClient with cosine HNSW). Both have import-guarded dependencies.

Purpose: These are the concrete implementations that the VectorSearchService (Plan 03) will wire together. SapBERT provides biomedical entity embeddings; ChromaDB provides persistent vector storage. Both must be fully lazy (no imports at module level, no model loading until first use).

Output: 2 implementation files, each extending its respective ABC from Plan 01.
</objective>

<execution_context>
@/Users/tyo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tyo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@lobster/core/vector/embeddings/base.py
@lobster/core/vector/backends/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SapBERTEmbedder with lazy loading and CLS pooling</name>
  <files>lobster/core/vector/embeddings/sapbert.py</files>
  <action>
Create `lobster/core/vector/embeddings/sapbert.py` implementing the `BaseEmbedder` ABC.

**Class: `SapBERTEmbedder`**

Constants:
- `MODEL_NAME = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"`
- `DIMENSIONS = 768`

`__init__(self)`:
- Store `self._model = None` (lazy). Do NOT import sentence_transformers or torch here.

`_load_model(self)`:
- Guard: if `self._model is not None`, return immediately.
- Inside this method (NOT at module level), import:
  ```python
  try:
      from sentence_transformers import SentenceTransformer
      from sentence_transformers.models import Transformer, Pooling
  except ImportError:
      raise ImportError(
          "SapBERT embeddings require sentence-transformers and PyTorch. "
          "Install with: pip install 'lobster-ai[vector-search]'"
      )
  ```
- Construct model with explicit CLS pooling (CRITICAL -- SapBERT was trained with CLS token, not mean pooling):
  ```python
  transformer = Transformer(self.MODEL_NAME)
  pooling = Pooling(
      word_embedding_dimension=self.DIMENSIONS,
      pooling_mode="cls"
  )
  self._model = SentenceTransformer(modules=[transformer, pooling])
  ```
- Log info: `logger.info("Loaded SapBERT model (%s). This is a one-time operation.", self.MODEL_NAME)`

`embed_text(self, text: str) -> list[float]`:
- Call `self._load_model()`
- `embedding = self._model.encode(text, convert_to_numpy=True)`
- Return `embedding.tolist()`

`embed_batch(self, texts: list[str]) -> list[list[float]]`:
- Call `self._load_model()`
- `embeddings = self._model.encode(texts, convert_to_numpy=True, batch_size=128)`
- Return `embeddings.tolist()`
- Note: batch_size=128 per SapBERT model card recommendation to avoid OOM

`dimensions` property:
- Return `self.DIMENSIONS` (768)

Use `logging.getLogger(__name__)` for the logger. Include a module docstring referencing the SapBERT model card and explaining the CLS pooling requirement.

CRITICAL RULES:
- NO module-level imports of sentence_transformers, transformers, or torch
- NO model loading in __init__ -- only on first embed_text/embed_batch call
- The import guard with helpful message is in _load_model(), not at module level
  </action>
  <verify>
Test 1 - Import succeeds without torch installed check:
`cd /Users/tyo/omics-os/lobster && python -c "import sys; pre = set(sys.modules.keys()); from lobster.core.vector.embeddings.sapbert import SapBERTEmbedder; post = set(sys.modules.keys()); new = post - pre; has_heavy = any(m in new for m in ['torch', 'sentence_transformers', 'transformers']); print('Import OK, heavy deps loaded:', has_heavy); e = SapBERTEmbedder(); print('Init OK, model loaded:', e._model is not None)"`

Expected: Import OK, heavy deps loaded: False. Init OK, model loaded: False.

Test 2 - Verify ABC contract:
`cd /Users/tyo/omics-os/lobster && python -c "from lobster.core.vector.embeddings.sapbert import SapBERTEmbedder; e = SapBERTEmbedder(); print('dimensions:', e.dimensions); print('name:', e.name)"`

Expected: dimensions: 768, name: SapBERTEmbedder
  </verify>
  <done>SapBERTEmbedder class created extending BaseEmbedder. Model loading is lazy (first use only). CLS-token pooling explicitly configured. Import guard raises ImportError with helpful install message. No module-level heavy imports.</done>
</task>

<task type="auto">
  <name>Task 2: Implement ChromaDBBackend with PersistentClient and cosine HNSW</name>
  <files>lobster/core/vector/backends/chromadb_backend.py</files>
  <action>
Create `lobster/core/vector/backends/chromadb_backend.py` implementing the `BaseVectorBackend` ABC.

**Class: `ChromaDBBackend`**

`__init__(self, persist_path: str | None = None)`:
- Default persist_path: `str(Path.home() / ".lobster" / "vector_store")` (per user decision)
- Create directory: `Path(persist_path).mkdir(parents=True, exist_ok=True)` (per Pitfall 6 from research)
- Lazy client: `self._persist_path = persist_path`, `self._client = None`

`_get_client(self)`:
- If `self._client is not None`, return it.
- Inside this method, import chromadb:
  ```python
  try:
      import chromadb
  except ImportError:
      raise ImportError(
          "ChromaDB vector store is required. "
          "Install with: pip install 'lobster-ai[vector-search]'"
      )
  ```
- `self._client = chromadb.PersistentClient(path=self._persist_path)`
- Return `self._client`

`_get_or_create_collection(self, name: str)`:
- `client = self._get_client()`
- Return `client.get_or_create_collection(name=name, metadata={"hnsw:space": "cosine"})`
- This ensures cosine distance HNSW for all collections (per user decision).

`add_documents(self, collection_name, ids, embeddings, documents=None, metadatas=None) -> None`:
- Get collection via `_get_or_create_collection(collection_name)`
- Call `collection.add(ids=ids, embeddings=embeddings, documents=documents, metadatas=metadatas)`
- Handle large batches: if len(ids) > 5000, process in chunks of 5000 (ChromaDB may have batch limits)

`search(self, collection_name, query_embedding, n_results=5) -> dict[str, Any]`:
- Get collection
- Return `collection.query(query_embeddings=[query_embedding], n_results=n_results, include=["documents", "metadatas", "distances"])`
- This returns ChromaDB's column-oriented format: `{ids: [[...]], distances: [[...]], metadatas: [[...]], documents: [[...]]}`

`delete(self, collection_name, ids) -> None`:
- Get collection
- `collection.delete(ids=ids)`

`count(self, collection_name) -> int`:
- Get collection
- Return `collection.count()`

`collection_exists(self, collection_name) -> bool`:
- Override the default from ABC with a more efficient approach:
- `client = self._get_client()`
- Try `client.get_collection(collection_name)` -> return True
- Except `ValueError` -> return False (ChromaDB raises ValueError for nonexistent collections)

Include a module docstring explaining the ChromaDB PersistentClient pattern and cosine distance (returns 1-similarity, conversion happens in the service layer).

CRITICAL RULES:
- NO module-level `import chromadb` -- all inside `_get_client()`
- Create persist directory BEFORE initializing PersistentClient
- Always use `metadata={"hnsw:space": "cosine"}` on collection creation
- The search method returns RAW ChromaDB results (distance->similarity conversion happens in VectorSearchService, Plan 03)
  </action>
  <verify>
Test 1 - Import without chromadb installed check:
`cd /Users/tyo/omics-os/lobster && python -c "import sys; pre = set(sys.modules.keys()); from lobster.core.vector.backends.chromadb_backend import ChromaDBBackend; post = set(sys.modules.keys()); new = post - pre; has_heavy = 'chromadb' in new; print('Import OK, chromadb loaded:', has_heavy)"`

Expected: Import OK, chromadb loaded: False

Test 2 - Verify ABC contract (if chromadb is installed):
`cd /Users/tyo/omics-os/lobster && python -c "
import tempfile, os
try:
    from lobster.core.vector.backends.chromadb_backend import ChromaDBBackend
    with tempfile.TemporaryDirectory() as td:
        b = ChromaDBBackend(persist_path=os.path.join(td, 'test_store'))
        print('collection_exists (empty):', b.collection_exists('test'))
        b.add_documents('test', ids=['1'], embeddings=[[0.1]*768], documents=['hello'], metadatas=[{'ontology_id': 'X'}])
        print('count:', b.count('test'))
        results = b.search('test', [0.1]*768, n_results=1)
        print('search ids:', results['ids'])
        b.delete('test', ids=['1'])
        print('count after delete:', b.count('test'))
        print('ALL TESTS PASSED')
except ImportError as e:
    print(f'chromadb not installed (expected in CI): {e}')
"`
  </verify>
  <done>ChromaDBBackend class created extending BaseVectorBackend. Uses PersistentClient with lazy initialization. All collections use cosine HNSW space. Persist directory auto-created. Import guard raises helpful message. Search returns raw ChromaDB column-oriented format.</done>
</task>

</tasks>

<verification>
1. `from lobster.core.vector.embeddings.sapbert import SapBERTEmbedder` succeeds without importing torch
2. `from lobster.core.vector.backends.chromadb_backend import ChromaDBBackend` succeeds without importing chromadb
3. `SapBERTEmbedder()` initializes without model download (lazy)
4. `SapBERTEmbedder().dimensions == 768`
5. ChromaDBBackend CRUD cycle works (add, count, search, delete) when chromadb is installed
6. ChromaDB collections use cosine HNSW space
</verification>

<success_criteria>
- SapBERTEmbedder with lazy loading, CLS pooling, 768d dimensions, batch_size=128
- ChromaDBBackend with PersistentClient, cosine HNSW, auto-directory creation
- Both import-guarded with "pip install lobster-ai[vector-search]" message
- No module-level heavy imports in either file
- Both correctly extend their respective ABCs from Plan 01
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
