---
phase: 06-automation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lobster/core/vector/embeddings/minilm.py
  - lobster/core/vector/embeddings/openai_embedder.py
  - lobster/core/vector/config.py
  - tests/unit/core/vector/test_embedders.py
  - tests/unit/core/vector/test_config.py
autonomous: true
requirements: [EMBED-03, EMBED-04]

must_haves:
  truths:
    - "Setting LOBSTER_EMBEDDING_PROVIDER=minilm creates a MiniLMEmbedder with 384 dimensions and mean pooling"
    - "Setting LOBSTER_EMBEDDING_PROVIDER=openai creates an OpenAIEmbedder with 1536 dimensions and lazy client init"
    - "Neither MiniLM nor OpenAI provider loads any model or creates any client at import time"
    - "Missing sentence-transformers or openai package raises ImportError with helpful install message"
  artifacts:
    - path: "lobster/core/vector/embeddings/minilm.py"
      provides: "MiniLMEmbedder implementing BaseEmbedder with mean pooling"
      exports: ["MiniLMEmbedder"]
    - path: "lobster/core/vector/embeddings/openai_embedder.py"
      provides: "OpenAIEmbedder implementing BaseEmbedder with lazy OpenAI client"
      exports: ["OpenAIEmbedder"]
    - path: "lobster/core/vector/config.py"
      provides: "Extended create_embedder() factory with minilm and openai branches"
    - path: "tests/unit/core/vector/test_embedders.py"
      provides: "Unit tests for MiniLM and OpenAI providers with mocked deps"
    - path: "tests/unit/core/vector/test_config.py"
      provides: "Updated factory tests for new embedder types"
  key_links:
    - from: "lobster/core/vector/config.py"
      to: "lobster/core/vector/embeddings/minilm.py"
      via: "lazy import in create_embedder()"
      pattern: "from lobster.core.vector.embeddings.minilm import MiniLMEmbedder"
    - from: "lobster/core/vector/config.py"
      to: "lobster/core/vector/embeddings/openai_embedder.py"
      via: "lazy import in create_embedder()"
      pattern: "from lobster.core.vector.embeddings.openai_embedder import OpenAIEmbedder"
---

<objective>
Add MiniLM (384d, mean pooling) and OpenAI (1536d, lazy client) embedding providers to the vector search infrastructure, wire them into the config factory, and add comprehensive unit tests.

Purpose: Provides general-purpose (MiniLM) and API-based (OpenAI) embedding alternatives to the biomedical-specific SapBERT, enabling vector search for non-biomedical text and users who prefer cloud embeddings.
Output: Two new embedder implementations, updated config factory, and unit tests.
</objective>

<execution_context>
@/Users/tyo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tyo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-automation/06-RESEARCH.md
@lobster/core/vector/embeddings/sapbert.py
@lobster/core/vector/embeddings/base.py
@lobster/core/vector/config.py
@lobster/core/schemas/search.py
@tests/unit/core/vector/test_embedders.py
@tests/unit/core/vector/test_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: MiniLM and OpenAI embedding providers</name>
  <files>
    lobster/core/vector/embeddings/minilm.py
    lobster/core/vector/embeddings/openai_embedder.py
    lobster/core/vector/config.py
  </files>
  <action>
Create two new embedder files following the exact SapBERT pattern in `lobster/core/vector/embeddings/sapbert.py`.

**MiniLM provider** (`minilm.py`):
- Class `MiniLMEmbedder(BaseEmbedder)` with `MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"` and `DIMENSIONS = 384`
- Lazy `_load_model()` with import-guarded `from sentence_transformers import SentenceTransformer`
- CRITICAL: MiniLM uses MEAN pooling (the SentenceTransformer default). Do NOT configure CLS pooling like SapBERT. Simply use `SentenceTransformer(self.MODEL_NAME)` without custom Transformer/Pooling modules.
- `embed_text()` calls `self._model.encode(text, convert_to_numpy=True).tolist()`
- `embed_batch()` calls `self._model.encode(texts, convert_to_numpy=True, batch_size=128).tolist()`
- ImportError message: "MiniLM embeddings require sentence-transformers. Install with: pip install sentence-transformers"
- Module docstring: explain this is a general-purpose fallback for non-biomedical text

**OpenAI provider** (`openai_embedder.py`):
- Class `OpenAIEmbedder(BaseEmbedder)` with `MODEL_NAME = "text-embedding-3-small"` and `DIMENSIONS = 1536`
- `__init__(self, model: str | None = None)` allowing model override via constructor
- Lazy `_get_client()` with import-guarded `from openai import OpenAI`
- Client created as `OpenAI()` (reads OPENAI_API_KEY from env automatically)
- `embed_text()`: `client.embeddings.create(model=self._model_name, input=text)` -> `response.data[0].embedding`
- `embed_batch()`: `client.embeddings.create(model=self._model_name, input=texts)` -> `[item.embedding for item in response.data]`
- ImportError message: "OpenAI embeddings require the openai package. Install with: pip install openai"
- Module docstring: explain lazy client init, OPENAI_API_KEY requirement

**Config factory update** (`config.py`):
- Add two new branches to `create_embedder()` before the final `raise ValueError`:
  - `if self.embedding_provider == EmbeddingProvider.minilm:` -> lazy import and return `MiniLMEmbedder()`
  - `if self.embedding_provider == EmbeddingProvider.openai:` -> lazy import and return `OpenAIEmbedder()`
- Update the ValueError message to list all three providers: "Available: sapbert, minilm, openai"
- Note: The `EmbeddingProvider` enum in `schemas/search.py` already has `minilm` and `openai` values. Do NOT modify the schema.
  </action>
  <verify>
`python -c "from lobster.core.vector.embeddings.minilm import MiniLMEmbedder; e = MiniLMEmbedder(); assert e.dimensions == 384; assert e._model is None; print('MiniLM OK')"` succeeds.
`python -c "from lobster.core.vector.embeddings.openai_embedder import OpenAIEmbedder; e = OpenAIEmbedder(); assert e.dimensions == 1536; assert e._client is None; print('OpenAI OK')"` succeeds.
`python -c "from lobster.core.vector.config import VectorSearchConfig; from lobster.core.schemas.search import EmbeddingProvider; c = VectorSearchConfig(embedding_provider=EmbeddingProvider.minilm); e = c.create_embedder(); assert e.dimensions == 384; print('Factory OK')"` succeeds.
  </verify>
  <done>
MiniLMEmbedder with 384d mean pooling, OpenAIEmbedder with 1536d lazy client, and config factory routing for both providers are functional. No model or client loading happens at import/construction time.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for MiniLM and OpenAI providers</name>
  <files>
    tests/unit/core/vector/test_embedders.py
    tests/unit/core/vector/test_config.py
  </files>
  <action>
**Append to `test_embedders.py`** (do NOT modify existing SapBERT tests):

Add `TestMiniLMEmbedder` class:
- `test_model_not_loaded_on_init`: MiniLMEmbedder()._model is None
- `test_dimensions_is_384`: embedder.dimensions == 384
- `test_model_name`: MiniLMEmbedder.MODEL_NAME == "sentence-transformers/all-MiniLM-L6-v2"
- `test_mean_pooling_not_cls`: Mock `sys.modules` for sentence_transformers, call `_load_model()`, verify SentenceTransformer was called with just model name (no custom Pooling module). The key difference from SapBERT: MiniLM uses `SentenceTransformer(MODEL_NAME)` directly, not `SentenceTransformer(modules=[transformer, pooling])`.
- `test_embed_text_with_mock`: Set embedder._model to mock, call embed_text("test"), verify encode called with correct args
- `test_embed_batch_with_mock`: Set embedder._model to mock, call embed_batch(["a","b"]), verify batch_size=128
- `test_import_error_helpful_message`: Patch builtins.__import__ to raise ImportError for sentence_transformers, verify error mentions "pip install"
- `test_is_base_embedder_subclass`: isinstance(MiniLMEmbedder(), BaseEmbedder)

Add `TestOpenAIEmbedder` class:
- `test_client_not_created_on_init`: OpenAIEmbedder()._client is None
- `test_dimensions_is_1536`: embedder.dimensions == 1536
- `test_model_name_default`: OpenAIEmbedder.MODEL_NAME == "text-embedding-3-small"
- `test_custom_model_name`: OpenAIEmbedder(model="text-embedding-3-large")._model_name == "text-embedding-3-large"
- `test_embed_text_with_mock`: Mock _get_client() to return mock client, verify client.embeddings.create called with model and input
- `test_embed_batch_with_mock`: Same pattern, verify input=list passed
- `test_import_error_helpful_message`: Patch sys.modules to remove openai, verify ImportError mentions "pip install openai"
- `test_is_base_embedder_subclass`: isinstance(OpenAIEmbedder(), BaseEmbedder)

**Update `test_config.py`**:
- Replace `test_create_embedder_unsupported` (which tests minilm raising ValueError) with `test_create_embedder_minilm`: verify VectorSearchConfig(embedding_provider=EmbeddingProvider.minilm).create_embedder() returns MiniLMEmbedder instance
- Add `test_create_embedder_openai`: verify VectorSearchConfig(embedding_provider=EmbeddingProvider.openai).create_embedder() returns OpenAIEmbedder instance
- Add `test_from_env_openai_embedder(monkeypatch)`: set LOBSTER_EMBEDDING_PROVIDER=openai, verify config.embedding_provider == EmbeddingProvider.openai

Use the same mocking patterns as existing SapBERT tests (sys.modules patching for lazy imports, numpy mock returns).
  </action>
  <verify>
`cd /Users/tyo/Omics-OS/lobster && python -m pytest tests/unit/core/vector/test_embedders.py tests/unit/core/vector/test_config.py -v --tb=short` -- all tests pass including new MiniLM and OpenAI tests.
  </verify>
  <done>
All MiniLM tests pass (8 tests: lazy loading, mean pooling verification, dimensions, batch_size, import guard, subclass check). All OpenAI tests pass (8 tests: lazy client, dimensions, custom model, embed mock, import guard, subclass check). Config factory tests updated to verify minilm and openai provider creation.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/unit/core/vector/ -v --tb=short` -- all vector tests pass (existing + new)
2. `python -c "from lobster.core.vector.embeddings.minilm import MiniLMEmbedder"` -- imports without loading model
3. `python -c "from lobster.core.vector.embeddings.openai_embedder import OpenAIEmbedder"` -- imports without creating client
4. `python -c "from lobster.core.vector.config import VectorSearchConfig; VectorSearchConfig()"` -- no import regression
</verification>

<success_criteria>
- MiniLMEmbedder produces 384d embeddings with mean pooling (not CLS)
- OpenAIEmbedder produces 1536d embeddings with lazy client init
- Config factory routes minilm and openai correctly
- All existing vector tests still pass (zero regressions)
- At least 16 new tests for the two providers
</success_criteria>

<output>
After completion, create `.planning/phases/06-automation/06-01-SUMMARY.md`
</output>
