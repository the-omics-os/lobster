---
phase: 06-automation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/build_ontology_embeddings.py
autonomous: true
requirements: [DATA-01, DATA-02, DATA-03, DATA-04]

must_haves:
  truths:
    - "Running the build script with --ontology mondo produces a mondo_sapbert_768.tar.gz tarball"
    - "Each term in the ChromaDB collection has metadata: term_id, name, synonyms, namespace, is_obsolete"
    - "Terms are embedded as '{label}: {definition}' with definition stripped of OBO quotes and references"
    - "Terms without a name are skipped; terms without a definition are embedded as label-only"
    - "The tarball contains a ChromaDB PersistentClient directory that can be restored by a new PersistentClient"
  artifacts:
    - path: "scripts/build_ontology_embeddings.py"
      provides: "Standalone CLI build script for ontology embeddings"
      min_lines: 100
  key_links:
    - from: "scripts/build_ontology_embeddings.py"
      to: "lobster/core/vector/embeddings/sapbert.py"
      via: "SapBERTEmbedder import for embedding generation"
      pattern: "from lobster.core.vector.embeddings.sapbert import SapBERTEmbedder"
    - from: "scripts/build_ontology_embeddings.py"
      to: "lobster/core/vector/ontology_graph.py"
      via: "OBO_URLS import for ontology URLs"
      pattern: "from lobster.core.vector.ontology_graph import OBO_URLS"
    - from: "scripts/build_ontology_embeddings.py"
      to: "lobster/core/vector/service.py"
      via: "ONTOLOGY_COLLECTIONS import for versioned collection names"
      pattern: "from lobster.core.vector.service import ONTOLOGY_COLLECTIONS"
---

<objective>
Create the offline build script that parses OBO ontology files, generates SapBERT embeddings, stores them in ChromaDB collections with full metadata, and produces gzipped tarballs ready for S3 upload.

Purpose: Eliminates the 10-15 minute cold-start embedding time for fresh installs by pre-building ontology embeddings offline. The tarballs are uploaded to S3 and auto-downloaded by end users.
Output: `scripts/build_ontology_embeddings.py` -- a standalone CLI script.
</objective>

<execution_context>
@/Users/tyo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tyo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-automation/06-RESEARCH.md
@lobster/core/vector/ontology_graph.py
@lobster/core/vector/service.py
@lobster/core/vector/embeddings/sapbert.py
@lobster/core/vector/backends/chromadb_backend.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create the build script</name>
  <files>
    scripts/build_ontology_embeddings.py
  </files>
  <action>
Create `scripts/` directory and the build script. The script is a standalone CLI tool run by a developer before releases. It is NOT imported by the runtime library.

**Structure:**
- Shebang line: `#!/usr/bin/env python3`
- Module docstring explaining purpose, usage, and output format
- Uses `argparse` for CLI (not Typer -- this is a standalone dev tool, minimal deps)

**CLI interface:**
```
python scripts/build_ontology_embeddings.py --ontology mondo --output-dir ./build_output
python scripts/build_ontology_embeddings.py --all --output-dir ./build_output
```

Arguments:
- `--ontology`: One of "mondo", "uberon", "cell_ontology" (from OBO_URLS keys)
- `--all`: Process all 3 ontologies
- `--output-dir`: Directory for ChromaDB persist dirs and tarballs (default: `./ontology_build_output`)
- Mutually exclusive: --ontology vs --all

**Core function `build_ontology(ontology_name, obo_url, output_dir)`:**

1. **Parse OBO**: Import obonet (import-guarded with helpful message). Call `obonet.read_obo(obo_url)`. Log term count.

2. **Extract terms with metadata**: For each node in `graph.nodes(data=True)`:
   - `name = data.get("name", "")` -- skip if empty (DATA-02: only embed terms with a name)
   - `definition = data.get("def", "")` -- strip OBO encoding: remove outer quotes and trailing `[references]` bracket. Use: `definition.strip('"').split('" [')[0].strip()` if definition is not empty.
   - Text to embed: `f"{name}: {definition}"` if definition is non-empty after stripping, else just `name` (DATA-02)
   - `synonyms = data.get("synonym", [])` -> join as `"; ".join(str(s) for s in synonyms)` for metadata
   - `namespace = data.get("namespace", "")`
   - `is_obsolete = str(data.get("is_obsolete", "false")).lower()`
   - Skip terms where `is_obsolete == "true"` (don't embed obsolete terms)

3. **Embed with SapBERT**: Import `SapBERTEmbedder` from `lobster.core.vector.embeddings.sapbert`. Create instance, call `embed_batch(texts)`. Log embedding progress (batch count).

4. **Store in ChromaDB**: Use `chromadb.PersistentClient(path=str(persist_dir))`. The persist_dir MUST be `output_dir / f"{ontology_name}_sapbert_768"`.
   - Collection name MUST match ONTOLOGY_COLLECTIONS versioned names: derive from `ONTOLOGY_COLLECTIONS[ontology_name]` (e.g., "mondo_v2024_01"). Import ONTOLOGY_COLLECTIONS from `lobster.core.vector.service`.
   - Use `hnsw:space = "cosine"` metadata (matching ChromaDBBackend pattern).
   - Batch add in chunks of 5000 (matching `_BATCH_SIZE` in chromadb_backend.py).
   - IDs: use `term_id` (e.g., "MONDO:0005575")
   - Documents: the text that was embedded
   - Metadatas: `{"term_id": node_id, "name": name, "synonyms": synonym_str, "namespace": namespace, "is_obsolete": is_obsolete}`
   - After adding, `del client` to close the connection before tarring.

5. **Create tarball**: Use `tarfile.open(tarball_path, "w:gz")` with `tar.add(persist_dir, arcname=persist_dir.name)`. Tarball name: `f"{ontology_name}_sapbert_768.tar.gz"` in output_dir (DATA-04).

6. **Log summary**: Print collection name, term count, tarball size, tarball path.

**Main function:**
- Parse args, resolve ontologies list
- Import OBO_URLS from `lobster.core.vector.ontology_graph`
- For each ontology: call `build_ontology(name, OBO_URLS[name], output_dir)`
- Print final summary with all tarball paths and sizes

**Error handling:**
- ImportError for obonet, chromadb, sentence-transformers: helpful install messages
- Network errors during OBO download: catch and log with URL
- Empty ontology (0 terms): warn and skip
  </action>
  <verify>
`python scripts/build_ontology_embeddings.py --help` prints usage without errors.
`python -c "import ast; ast.parse(open('scripts/build_ontology_embeddings.py').read()); print('Syntax OK')"` succeeds.
  </verify>
  <done>
Build script exists at scripts/build_ontology_embeddings.py, accepts --ontology/--all/--output-dir, parses OBO via obonet, embeds with SapBERT, stores in ChromaDB with full metadata (term_id, name, synonyms, namespace, is_obsolete), and produces tar.gz tarballs with correct naming convention matching ONTOLOGY_COLLECTIONS versioned names.
  </done>
</task>

<task type="auto">
  <name>Task 2: Smoke test the build script with a dry-run validation</name>
  <files>
    scripts/build_ontology_embeddings.py
  </files>
  <action>
Add a `--dry-run` flag to the build script that validates the full pipeline WITHOUT downloading the full OBO files or running SapBERT.

**Dry-run behavior:**
- Parse args as normal
- For each ontology: print the OBO URL, the output paths (persist_dir, tarball path), and the versioned collection name
- Verify all imports work: obonet, chromadb, SapBERTEmbedder (attempt import, catch ImportError, report)
- Verify output directory is writable (create if needed)
- Print a summary: "Dry run complete. Ready to build {N} ontologies. Run without --dry-run to proceed."

This ensures the script is syntactically correct and all dependencies are importable before committing to the 10-15 min full build.

Also add a `--version-tag` argument (default: "v2024_01") that is used in the collection name suffix. This way the versioned collection name matches `ONTOLOGY_COLLECTIONS` by default, but can be changed for future ontology releases without modifying the script.

Update the collection name derivation to use: `f"{ontology_name}_{version_tag.replace('.', '_')}"` but ALSO validate it matches ONTOLOGY_COLLECTIONS if the default version tag is used (log a warning if mismatch).
  </action>
  <verify>
`python scripts/build_ontology_embeddings.py --dry-run --all` runs successfully, prints OBO URLs, output paths, collection names, and dependency check results without downloading anything or loading any models.
  </verify>
  <done>
Build script has --dry-run flag for validation, --version-tag for future-proofing, and validates all dependencies and paths before any expensive operations. Dry-run confirms: imports work, output dir writable, collection names match ONTOLOGY_COLLECTIONS.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/build_ontology_embeddings.py --help` shows usage with --ontology, --all, --output-dir, --dry-run, --version-tag
2. `python scripts/build_ontology_embeddings.py --dry-run --all` validates imports and paths
3. Script references correct ONTOLOGY_COLLECTIONS names (mondo_v2024_01, uberon_v2024_01, cell_ontology_v2024_01)
4. Script uses SapBERTEmbedder (not MiniLM or OpenAI) for ontology building
</verification>

<success_criteria>
- Build script parses OBO files via obonet and generates SapBERT embeddings (DATA-01)
- Text format is "{label}: {definition}" with cleaned OBO definitions, not synonyms (DATA-02)
- ChromaDB collections have all 5 metadata fields: term_id, name, synonyms, namespace, is_obsolete (DATA-03)
- Tarballs follow naming convention: {ontology}_sapbert_768.tar.gz (DATA-04)
- Dry-run validates the full pipeline without expensive operations
</success_criteria>

<output>
After completion, create `.planning/phases/06-automation/06-02-SUMMARY.md`
</output>
