---
phase: 06-automation
plan: 03
type: execute
wave: 2
depends_on: ["06-02"]
files_modified:
  - lobster/core/vector/backends/chromadb_backend.py
  - tests/unit/core/vector/test_auto_download.py
autonomous: true
requirements: [DATA-05, DATA-06]

must_haves:
  truths:
    - "When querying an ontology collection that is empty/missing, ChromaDB backend checks ~/.lobster/ontology_cache/ for a cached tarball"
    - "If no cached tarball, backend downloads it from S3 with a Rich progress bar showing speed and ETA"
    - "Downloaded tarball is extracted to cache and collection data loaded into the backend persist path"
    - "Second query for same ontology collection hits ChromaDB directly with no download or extraction"
    - "Network failure during download produces a clear error message, not a corrupted cache"
  artifacts:
    - path: "lobster/core/vector/backends/chromadb_backend.py"
      provides: "Auto-download extension with S3 tarball fetching and cache management"
      contains: "ONTOLOGY_TARBALLS"
    - path: "tests/unit/core/vector/test_auto_download.py"
      provides: "Unit tests for auto-download, cache hit, download failure, tarball extraction"
      min_lines: 60
  key_links:
    - from: "lobster/core/vector/backends/chromadb_backend.py"
      to: "https://lobster-ontology-data.s3.amazonaws.com/v1/"
      via: "ONTOLOGY_TARBALLS URL mapping"
      pattern: "ONTOLOGY_TARBALLS"
    - from: "lobster/core/vector/backends/chromadb_backend.py"
      to: "lobster/core/vector/service.py"
      via: "Collection names match ONTOLOGY_COLLECTIONS versioned names"
      pattern: "mondo_v2024_01|uberon_v2024_01|cell_ontology_v2024_01"
---

<objective>
Extend the ChromaDB backend to auto-download pre-built ontology tarballs from S3 on first use, with caching at ~/.lobster/ontology_cache/ and a Rich progress bar for download feedback.

Purpose: Eliminates the 10-15 minute cold-start for fresh installs. Users get prebuilt embeddings downloaded in <60s instead of embedding 60K+ terms locally.
Output: Extended ChromaDB backend with auto-download, cache dir, and comprehensive tests.
</objective>

<execution_context>
@/Users/tyo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tyo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-automation/06-RESEARCH.md
@.planning/phases/06-automation/06-02-SUMMARY.md
@lobster/core/vector/backends/chromadb_backend.py
@lobster/core/vector/service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Auto-download extension for ChromaDB backend</name>
  <files>
    lobster/core/vector/backends/chromadb_backend.py
  </files>
  <action>
Extend the existing ChromaDBBackend class with ontology tarball auto-download. All additions go into the existing file -- do NOT create a separate module.

**Add module-level constants** (after `_BATCH_SIZE`):

```python
ONTOLOGY_TARBALLS: dict[str, str] = {
    "mondo_v2024_01": "https://lobster-ontology-data.s3.amazonaws.com/v1/mondo_sapbert_768.tar.gz",
    "uberon_v2024_01": "https://lobster-ontology-data.s3.amazonaws.com/v1/uberon_sapbert_768.tar.gz",
    "cell_ontology_v2024_01": "https://lobster-ontology-data.s3.amazonaws.com/v1/cell_ontology_sapbert_768.tar.gz",
}

ONTOLOGY_CACHE_DIR = Path.home() / ".lobster" / "ontology_cache"
```

**Add `_download_with_progress(url, dest)` module-level function:**
- Import `requests` (already a core dependency) and `rich.progress` components
- Stream download with `requests.get(url, stream=True)`
- Use Rich Progress with BarColumn, DownloadColumn, TransferSpeedColumn, TimeRemainingColumn
- Download to a temporary file (`dest.with_suffix('.tmp')`) first, then rename to `dest` only on success (atomic move pattern to prevent partial corruption per research pitfall #4)
- Clean up .tmp file in a `finally` block on error
- Raise on HTTP errors with clear message including URL

**Add `_ensure_ontology_data(self, collection_name)` method to ChromaDBBackend:**
1. Check if `collection_name` is in `ONTOLOGY_TARBALLS`. If not, return False (not an ontology, skip).
2. Check if collection already exists AND has documents: `self.collection_exists(collection_name)` and `self.count(collection_name) > 0`. If yes, return True (already populated).
3. Derive tarball filename from URL (e.g., "mondo_sapbert_768.tar.gz").
4. Check `ONTOLOGY_CACHE_DIR / tarball_filename`. If exists, extract and load (skip download).
5. If not cached: download tarball from S3 URL to `ONTOLOGY_CACHE_DIR / tarball_filename` using `_download_with_progress()`.
6. **Extract tarball**: Use `tarfile.open(tarball_path)` with `tar.extractall(path=temp_dir, filter="data")` (Python 3.12+ safe extraction per research pitfall #3). Extract to a `tempfile.mkdtemp()` first.
7. **Load extracted data into the backend's persist path**: The tarball contains a directory like `mondo_sapbert_768/` which is a full ChromaDB PersistentClient directory. Create a NEW temporary PersistentClient pointing at the extracted dir, get the collection, read all data, then add to the backend's own collection at `self._persist_path`. This avoids SQLite locking issues from two clients sharing a directory (research pitfall #2). Specifically:
   - `source_client = chromadb.PersistentClient(path=str(extracted_dir))`
   - `source_coll = source_client.get_collection(collection_name)`
   - `data = source_coll.get(include=["embeddings", "documents", "metadatas"])`
   - `self.add_documents(collection_name, ids=data["ids"], embeddings=data["embeddings"], documents=data["documents"], metadatas=data["metadatas"])`
   - `del source_client`
   - Clean up temp extraction dir
8. Return True on success, log info messages throughout.

**Modify `_get_or_create_collection(self, name)` method:**
- BEFORE the existing `get_or_create_collection` call, add: `self._ensure_ontology_data(name)`
- This ensures that when any operation (search, add, count) accesses an ontology collection, the data is auto-downloaded if needed.
- Keep the rest of the method unchanged.

**Error handling:**
- Network errors (requests.ConnectionError, requests.Timeout): log warning "Failed to download ontology data for {name}: {error}. Falling back to empty collection." Do NOT crash -- return False from _ensure_ontology_data so the search proceeds with an empty collection.
- Tarball extraction errors (tarfile.ReadError): log warning, delete corrupt tarball from cache, return False.
- ChromaDB data loading errors: log warning, return False.
  </action>
  <verify>
`python -c "from lobster.core.vector.backends.chromadb_backend import ChromaDBBackend, ONTOLOGY_TARBALLS, ONTOLOGY_CACHE_DIR; assert len(ONTOLOGY_TARBALLS) == 3; print('Constants OK')"` succeeds.
`python -c "from lobster.core.vector.backends.chromadb_backend import ChromaDBBackend; b = ChromaDBBackend(); assert hasattr(b, '_ensure_ontology_data'); print('Method OK')"` succeeds (using /tmp to avoid creating files in default path).
  </verify>
  <done>
ChromaDB backend has _ensure_ontology_data() method that checks cache, downloads from S3 with Rich progress, extracts tarball safely, and loads data into the backend. Network failures degrade gracefully to empty collections.
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for auto-download and cache management</name>
  <files>
    tests/unit/core/vector/test_auto_download.py
  </files>
  <action>
Create a new test file for auto-download functionality. Tests must NOT make real network calls or require chromadb.

**Test class `TestOntologyTarballs`:**
- `test_tarball_urls_cover_all_three_ontologies`: ONTOLOGY_TARBALLS has exactly 3 entries with keys mondo_v2024_01, uberon_v2024_01, cell_ontology_v2024_01
- `test_tarball_urls_use_correct_s3_bucket`: All URLs start with "https://lobster-ontology-data.s3.amazonaws.com/v1/"
- `test_tarball_urls_use_correct_filenames`: URLs end with mondo_sapbert_768.tar.gz, uberon_sapbert_768.tar.gz, cell_ontology_sapbert_768.tar.gz
- `test_cache_dir_is_under_home_lobster`: ONTOLOGY_CACHE_DIR ends with ".lobster/ontology_cache"

**Test class `TestEnsureOntologyData`:**
Use a ChromaDBBackend with `tmp_path` persist path. Mock chromadb and requests.

- `test_returns_false_for_non_ontology_collection`: _ensure_ontology_data("my_custom_collection") returns False immediately
- `test_returns_true_if_collection_already_populated`: Mock collection_exists=True and count=N>0, verify no download attempted
- `test_downloads_tarball_on_cache_miss`: Mock requests.get to return a fake response, verify _download_with_progress is called with correct URL
- `test_skips_download_if_tarball_cached`: Create a fake tarball file in cache dir, mock tarfile.open, verify requests.get NOT called
- `test_graceful_failure_on_network_error`: Mock requests.get to raise ConnectionError, verify returns False (not raises), verify logger.warning called
- `test_graceful_failure_on_corrupt_tarball`: Mock tarfile.open to raise ReadError, verify returns False, verify corrupt file deleted from cache
- `test_atomic_download_cleans_up_tmp_on_failure`: Mock requests.get to raise mid-download, verify no .tmp file left behind

**Test class `TestDownloadWithProgress`:**
- `test_downloads_to_temp_then_renames`: Mock requests.get with streaming response, verify final file exists at dest (not .tmp)
- `test_http_error_raises`: Mock response.raise_for_status to raise, verify exception propagated

All tests should skip if chromadb not installed (`pytest.importorskip("chromadb")` at module level or per-test skipif).

Use `tmp_path` fixture extensively for isolated file operations. Mock `ONTOLOGY_CACHE_DIR` to point to tmp_path subdirectory for test isolation.
  </action>
  <verify>
`cd /Users/tyo/Omics-OS/lobster && python -m pytest tests/unit/core/vector/test_auto_download.py -v --tb=short` -- all tests pass.
  </verify>
  <done>
Auto-download tests cover: tarball URL correctness, cache directory location, cache hit/miss paths, graceful network failure, corrupt tarball handling, atomic download cleanup. All tests use mocked dependencies with no real network calls.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/unit/core/vector/ -v --tb=short` -- all vector tests pass including new auto-download tests
2. `python -c "from lobster.core.vector.backends.chromadb_backend import ONTOLOGY_TARBALLS; print(ONTOLOGY_TARBALLS)"` shows 3 S3 URLs
3. Existing ChromaDB backend tests still pass (zero regressions)
4. No import-time side effects: `python -c "import lobster.core.vector"` completes in <500ms
</verification>

<success_criteria>
- ChromaDB backend auto-downloads tarballs from S3 on first use to ~/.lobster/ontology_cache/ (DATA-05)
- S3 URLs point to s3://lobster-ontology-data/v1/ with correct tarball names (DATA-06)
- Rich progress bar shows during download
- Network failures degrade gracefully (warning + empty collection, no crash)
- Atomic download prevents partial file corruption
- At least 10 auto-download unit tests with mocked network
</success_criteria>

<output>
After completion, create `.planning/phases/06-automation/06-03-SUMMARY.md`
</output>
