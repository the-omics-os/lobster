# Profile Selection Implementation for `lobster init`

## Summary

Fixed bug where `LOBSTER_PROFILE` was never set during `lobster init`, causing all users to default to "production" profile. Now users can easily select their desired agent configuration profile during setup.

## What Was Implemented

### 1. **Conditional Profile Selection (Provider-Aware)**

Profile selection is **only shown for Anthropic and Bedrock providers** (those using Claude models). Ollama users see a note that profiles don't apply to local models.

### 2. **Interactive Mode**

When running `lobster init` interactively, users now see:

```
‚öôÔ∏è  Agent Configuration Profile
Choose which Claude models to use for analysis:

  1 - Development  (Haiku 4.5 - fastest, most affordable)
  2 - Production   (Sonnet 4 - balanced quality & speed) [recommended]
  3 - Ultra        (Sonnet 4.5 - highest quality, slower)
  4 - Godmode      (Opus 4.1 - experimental, most expensive)

üí° Tip: Development profile is great for testing, Production for real analysis
   You can change this later by editing .env or using the --profile flag

Choose profile [1/2/3/4] (2):
```

This appears **after provider setup** and **before NCBI keys**.

### 3. **Non-Interactive Mode**

Added `--profile` flag for CI/CD and automation:

```bash
# Anthropic with development profile
lobster init --non-interactive \
  --anthropic-key=sk-ant-xxx \
  --profile=development

# Bedrock with ultra profile
lobster init --non-interactive \
  --bedrock-access-key=AKIA... \
  --bedrock-secret-key=xxx \
  --profile=ultra

# Ollama (profile not applicable)
lobster init --non-interactive \
  --use-ollama
```

**Defaults:**
- Anthropic/Bedrock: `production` profile (if --profile not specified)
- Ollama: No profile written to .env

### 4. **Profile Details**

| Profile | Primary Model | Use Case | Cost |
|---------|--------------|----------|------|
| **development** | Claude Haiku 4.5 | Testing, rapid iteration | Lowest |
| **production** | Claude Sonnet 4 | Real analysis, balanced | Medium |
| **ultra** | Claude Sonnet 4.5 | Highest quality needed | Higher |
| **godmode** | Claude Opus 4.1 | Experimental, bleeding edge | Highest |

## Implementation Details

### Files Modified

**`lobster/cli.py` (lines 3106-3554):**
1. Added `--profile` parameter to `init()` command signature
2. Updated docstring with profile examples
3. Added profile validation in non-interactive mode (lines 3262-3283)
4. Added profile writing in non-interactive mode (lines 3305-3310)
5. Added profile selection UI in interactive mode (lines 3518-3554)

### Key Design Decisions

1. **Provider-Specific Behavior:**
   - Anthropic/Bedrock ‚Üí Profile selection shown, `LOBSTER_PROFILE` written
   - Ollama ‚Üí Profile skipped, informational note shown, no env var written

2. **Why Profiles Don't Apply to Ollama:**
   - Profiles reference Claude model ARNs (e.g., `us.anthropic.claude-haiku-4-5-*`)
   - Ollama uses completely different models (e.g., `llama3:8b-instruct`)
   - `LLMFactory` auto-detects and uses best available Ollama model
   - See `llm_factory.py:353-363` for auto-selection logic

3. **Error Handling:**
   - Invalid profile names ‚Üí Clear error with valid options
   - Profile with Ollama ‚Üí Warning (ignored gracefully)
   - No provider ‚Üí Error before profile check

## Testing Results

**Test Suite:** `test_profile_init.sh` (8 tests)

```
‚úì Anthropic default profile (production)
‚úì Anthropic with development profile
‚úì Anthropic with ultra profile
‚úì Anthropic with godmode profile
‚úì Bedrock default profile (production)
‚úì Bedrock with development profile
‚úì Ollama without profile (no LOBSTER_PROFILE in .env)
‚úì Ollama with profile flag (warning + ignored)

Result: 8/8 PASSED ‚úÖ
```

### Example Generated .env Files

**Anthropic with development profile:**
```bash
# Lobster AI Configuration
# Generated by lobster init --non-interactive

LOBSTER_LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-xxx

# Agent Configuration Profile
# Determines which Claude models are used for analysis
LOBSTER_PROFILE=development
```

**Ollama (no profile):**
```bash
# Lobster AI Configuration
# Generated by lobster init --non-interactive

LOBSTER_LLM_PROVIDER=ollama
```

## Usage Examples

### Interactive Mode

```bash
# Standard interactive setup
lobster init

# Reconfigure existing setup
lobster init --force
```

User will be prompted to:
1. Choose provider (Anthropic/Bedrock/Ollama)
2. Enter API keys
3. **[NEW]** Choose profile (only for Anthropic/Bedrock)
4. Optionally add NCBI keys
5. Optionally configure cloud access

### Non-Interactive Mode (CI/CD)

```bash
# Development environment
lobster init --non-interactive \
  --anthropic-key=$ANTHROPIC_KEY \
  --profile=development

# Production environment
lobster init --non-interactive \
  --bedrock-access-key=$AWS_KEY \
  --bedrock-secret-key=$AWS_SECRET \
  --profile=production

# Local testing with Ollama
lobster init --non-interactive \
  --use-ollama
```

### Changing Profile Later

Users have multiple options:

1. **Re-run init:** `lobster init --force`
2. **Edit .env manually:** Change `LOBSTER_PROFILE=development`
3. **Environment variable:** `export LOBSTER_PROFILE=ultra`
4. **Runtime flag:** `lobster chat --profile godmode` (if supported)

## Architecture Context

### Model ID Translation

`LLMFactory` (`lobster/config/llm_factory.py`) handles provider-specific model IDs:

| Provider | Model ID Format | Example |
|----------|----------------|---------|
| Bedrock | `us.anthropic.claude-<model>-<version>-v1:0` | `us.anthropic.claude-haiku-4-5-20251001-v1:0` |
| Anthropic | `claude-<model>-<version>` | `claude-haiku-4-5-20251001` |
| Ollama | `<model>:<tag>` | `llama3:8b-instruct` |

### Profile ‚Üí Model Mapping

Profiles defined in `lobster/config/agent_config.py` (`MODEL_PRESETS`):

```python
"claude-4-5-haiku": ModelConfig(
    provider=ModelProvider.BEDROCK_ANTHROPIC,
    model_id="us.anthropic.claude-haiku-4-5-20251001-v1:0",
    tier=ModelTier.ULTRA,
    temperature=1.0,
    region="us-east-1",
    description="Claude 4.5 haiku for development and worker agents",
    supports_thinking=True,
),
```

`LLMFactory.create_llm()` automatically translates Bedrock ARNs to Anthropic Direct API format when needed.

## Future Enhancements

Potential improvements (not in scope):

1. **Profile auto-detection:** Suggest profile based on available resources (RAM, GPU)
2. **Cost estimation:** Show estimated costs per 1M tokens for each profile
3. **Custom profiles:** Allow users to define custom model combinations
4. **Profile validation:** Check if Bedrock region has requested model available
5. **Interactive preview:** Show which models each agent will use before confirming

## Related Documentation

- **Agent Config:** `lobster/config/agent_config.py` (profiles, models)
- **LLM Factory:** `lobster/config/llm_factory.py` (provider detection, translation)
- **Provider Setup:** `lobster/config/provider_setup.py` (pure provider logic)
- **Configuration Guide:** `lobster/config/README_CONFIGURATION.md`

## Migration Guide

**For Existing Users:**

Existing `.env` files without `LOBSTER_PROFILE` will continue to work (defaults to `production` in `agent_config.py:269`).

To adopt the new feature:
```bash
# Add to existing .env
echo "LOBSTER_PROFILE=development" >> .env

# Or reconfigure completely
lobster init --force
```

**For New Users:**

Profile selection is now part of the standard setup flow. Just run:
```bash
lobster init
```

## Verification

To verify profile is active:
```bash
# Check .env file
grep LOBSTER_PROFILE .env

# Run with verbose logging
lobster chat --verbose
# Look for log line: "Creating LLM for agent 'supervisor' using provider: bedrock"
```

## Rollback

If issues arise:
```bash
# Remove profile from .env (falls back to production)
sed -i '' '/LOBSTER_PROFILE/d' .env

# Or revert to previous version
git checkout <previous-commit> lobster/cli.py
```

---

**Implementation Date:** 2025-12-07
**Test Coverage:** 8/8 non-interactive tests passing
**Breaking Changes:** None (backward compatible)
**Status:** ‚úÖ Production Ready
